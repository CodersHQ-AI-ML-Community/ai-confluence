# Become LLM Zero to Hero! (In progress)

## Week 1-2: Introduction to Natural Language Processing (NLP) and its applications

- Understand the basics of NLP and its subfields
- Learn about text preprocessing, tokenization, and normalization
- Explore common NLP tasks like sentiment analysis, named entity recognition, and machine translation
- Refer Playlists: https://www.youtube.com/playlist?list=PLv3UF7F3axVQ8G34U6pBxZn2gkwX2_xBs, https://www.youtube.com/playlist?list=PLeo1K3hjS3uuvuAXhYjV2lMEShq2UYSwX

## Week 3-4: Neural Networks and Deep Learning for NLP

- Learn the fundamentals of neural networks and deep learning
- Understand the architecture of common neural networks used in NLP, such as Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformers
- Practice implementing basic neural networks using frameworks like TensorFlow or PyTorch
- Refer Playlist: https://www.youtube.com/playlist?list=PLv3UF7F3axVQt18QNRShqnW6oeT3_6RFD

## Week 5-6: Transformer Architecture and Attention Mechanism

- Dive deep into the Transformer architecture and its components
- Understand the attention mechanism and its variants (e.g., self-attention, multi-head attention)
- Implement a basic Transformer model and experiment with different hyperparameters
- Refer Playlists: https://www.youtube.com/playlist?list=PLv3UF7F3axVT65U8TAOjWJ8Cwp3kyclzt, https://youtu.be/rPFkX5fJdRY?list=PLTl9hO2Oobd9pHnY0tlxxoMHAGlNuJIdR

## Week 7-10: Pre-training and Fine-tuning Techniques of Large Language Models

- Learn about pre-training techniques like causal language modeling and masked language modeling
- Understand the concept of transfer learning and fine-tuning pre-trained models for specific tasks
- Practice fine-tuning pre-trained models like BERT, GPT, or T5 on downstream tasks
- Learn how to build a large language model from scratch
- Refer Playlists: https://www.youtube.com/playlist?list=PLxqBkZuBynVQaqvEwN-qAjkNAJ6NgyfcM, https://www.youtube.com/watch?v=UU1WVnMk4E8&t=3142s&ab_channel=freeCodeCamp.org

## Week 11: Tokenization and Subword Units:

- Learn how model predicts the next tokens
- Study different tokenization approaches, such as word-level, character-level, and subword tokenization.
- Understand the importance of subword units (e.g., byte-pair encoding) in handling out-of-vocabulary words.
- Explore various tokenization libraries, such as Tiktoken, SentencePiece and WordPiece.
- Refer Playlist: https://www.youtube.com/watch?v=7N72dvQ7lDg&ab_channel=AIMakerspace, https://www.youtube.com/watch?v=zduSFxRajkE&t=311s&ab_channel=AndrejKarpathy
