{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6c9016",
   "metadata": {},
   "source": [
    "# Introduction to Decision Trees\n",
    "\n",
    "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It models decisions and their possible consequences, including chance event outcomes, resource costs, and utility. Decision Trees are easy to interpret, handle both numerical and categorical data, and can model complex relationships.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Nodes**: Each node represents a feature (attribute), and a condition is applied to that feature.\n",
    "- **Root Node**: The top node in a decision tree, representing the best predictor.\n",
    "- **Internal Nodes**: Nodes that have branches (child nodes).\n",
    "- **Leaf Nodes (Terminal Nodes)**: Nodes that do not have any children. They represent the output class label (for classification) or a continuous value (for regression).\n",
    "- **Branches**: Each branch represents the outcome of a decision or test. It connects nodes based on the outcome of a condition.\n",
    "- **Splits**: The process of dividing a node into two or more sub-nodes based on a feature value. The goal is to create subsets of data that are as homogeneous as possible.\n",
    "- **Entropy**: A measure of randomness or impurity in the dataset. Lower entropy means higher purity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d046d",
   "metadata": {},
   "source": [
    "## Entropy and Information Gain\n",
    "\n",
    "### Entropy\n",
    "Entropy is a measure of randomness or impurity in the dataset. Lower entropy means higher purity.\n",
    "\n",
    "\n",
    "### Information Gain (IG)\n",
    "Information Gain is the reduction in entropy after a dataset is split on a feature. Higher information gain indicates a more effective feature for splitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "950f13da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://th.bing.com/th/id/OIP.pZo_Izi8-qfjunv43s5fzQAAAA?rs=1&pid=ImgDetMain\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='https://th.bing.com/th/id/OIP.pZo_Izi8-qfjunv43s5fzQAAAA?rs=1&pid=ImgDetMain')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d394bf8d",
   "metadata": {},
   "source": [
    "### Steps to Build a Decision Tree\n",
    "\n",
    "1. **Select the Best Feature**: Choose the feature that provides the highest information gain for splitting the dataset.\n",
    "2. **Split the Data**: Divide the dataset into subsets based on the best feature.\n",
    "3. **Create Decision Nodes**: Recursively split the subsets, creating decision nodes until the stopping criteria are met (maximum depth, minimum samples, or pure nodes).\n",
    "\n",
    "### Stopping Criteria\n",
    "\n",
    "- **Maximum Depth**: The tree stops growing when it reaches the specified maximum depth.\n",
    "- **Minimum Samples per Split**: The tree stops growing if the number of samples in a node is less than the specified minimum.\n",
    "- **Pure Nodes**: The tree stops splitting if all samples in a node belong to the same class.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28036eb1",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d47226bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Define a class Node to represent each node in the decision tree\n",
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature      # Feature index used for splitting\n",
    "        self.threshold = threshold  # Threshold value for the split\n",
    "        self.left = left            # Left child node\n",
    "        self.right = right          # Right child node\n",
    "        self.value = value          # Value for leaf node (class label)\n",
    "\n",
    "        \n",
    "    def is_leaf_node(self):\n",
    "        return self.value is not None  # Check if the node is a leaf node (no further splits)\n",
    "    \n",
    "\n",
    "# Define the DecisionTree class for the decision tree classifier\n",
    "class DecisionTree:\n",
    "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
    "        self.min_samples_split = min_samples_split  # Minimum samples required to split\n",
    "        self.max_depth = max_depth                 # Maximum depth of the tree\n",
    "        self.n_features = n_features               # Number of features to consider for splitting\n",
    "        self.root = None                           # Root of the decision tree\n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features)\n",
    "       \n",
    "    # Set n_features to number of features in X if not specified, otherwise use specified or min of both\n",
    "        self.root = self._grow_tree(X, y)  # Start growing the tree from the root\n",
    "\n",
    "        \n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_feats = X.shape              # Get the number of samples and features\n",
    "        n_labels = len(np.unique(y))              # Get the number of unique labels\n",
    "\n",
    "        # Check the stopping criteria for recursion\n",
    "        if (depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
    "            leaf_value = self._most_common_label(y)  # Get the most common label in current node\n",
    "            return Node(value=leaf_value)            # Return a leaf node\n",
    "\n",
    "        \n",
    "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)  # Randomly select features to consider\n",
    "\n",
    "        \n",
    "        # Find the best feature and threshold for splitting\n",
    "        best_feature, best_thresh = self._best_split(X, y, feat_idxs)\n",
    "\n",
    "        # Create child nodes\n",
    "        left_idxs, right_idxs = self._split(X[:, best_feature], best_thresh)    # Split the data\n",
    "        left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth + 1)        # Recursively grow left subtree\n",
    "        right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth + 1)     # Recursively grow right subtree\n",
    "        return Node(best_feature, best_thresh, left, right)                     # Return the current node\n",
    "\n",
    "    \n",
    "    def _best_split(self, X, y, feat_idxs):\n",
    "        best_gain = -1  # Initialize best gain to a negative value\n",
    "        split_idx, split_threshold = None, None  # Initialize best feature index and threshold\n",
    "\n",
    "        for feat_idx in feat_idxs:\n",
    "            X_column = X[:, feat_idx]          # Get the feature values for the current feature index\n",
    "            thresholds = np.unique(X_column)   # Get unique values in the feature column\n",
    "\n",
    "            for thr in thresholds:\n",
    "                # Calculate the information gain for each threshold\n",
    "                gain = self._information_gain(y, X_column, thr)\n",
    "\n",
    "                if gain > best_gain:           # Update best gain if current gain is better\n",
    "                    best_gain = gain\n",
    "                    split_idx = feat_idx\n",
    "                    split_threshold = thr\n",
    "\n",
    "        return split_idx, split_threshold      # Return the best feature index and threshold\n",
    "\n",
    "    \n",
    "    def _information_gain(self, y, X_column, threshold):\n",
    "        parent_entropy = self._entropy(y)     # Calculate the entropy of the parent node\n",
    "\n",
    "        # Create child nodes\n",
    "        left_idxs, right_idxs = self._split(X_column, threshold)\n",
    "\n",
    "        if len(left_idxs) == 0 or len(right_idxs) == 0:  # If no split, return 0 gain\n",
    "            return 0\n",
    "\n",
    "        # Calculate the weighted average entropy of the children\n",
    "        n = len(y)\n",
    "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
    "        e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_l / n) * e_l + (n_r / n) * e_r\n",
    "\n",
    "        # Calculate the information gain\n",
    "        information_gain = parent_entropy - child_entropy\n",
    "        return information_gain\n",
    "\n",
    "    \n",
    "    def _split(self, X_column, split_thresh):\n",
    "        left_idxs = np.argwhere(X_column <= split_thresh).flatten()  # Indices of samples <= threshold\n",
    "        right_idxs = np.argwhere(X_column > split_thresh).flatten()  # Indices of samples > threshold\n",
    "        return left_idxs, right_idxs                                 # Return the indices of the split\n",
    "\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        hist = np.bincount(y)                                     # Count the occurrences of each class\n",
    "        ps = hist / len(y)                                        # Calculate the probabilities\n",
    "        return -np.sum([p * np.log(p) for p in ps if p > 0])      # Calculate and return the entropy\n",
    "\n",
    "    \n",
    "    def _most_common_label(self, y):\n",
    "        counter = Counter(y)                                      # Count the occurrences of each label\n",
    "        value = counter.most_common(1)[0][0]                      # Get the most common label\n",
    "        return value\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])  # Traverse the tree for each sample\n",
    "\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf_node():                               # If the node is a leaf, return its value\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:                 # Traverse the left or right subtree based on the feature value\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39982a01",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41872835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "# Importing required libraries and modules\n",
    "from sklearn import datasets                          # Importing datasets module from scikit-learn\n",
    "from sklearn.model_selection import train_test_split  # Importing train_test_split function from model_selection module\n",
    "import numpy as np                                    # Importing numpy library and aliasing it as np\n",
    "from DecisionTree import DecisionTree                 # Importing DecisionTree class from DecisionTree module\n",
    "\n",
    "\n",
    "# Loading the Breast Cancer dataset\n",
    "data = datasets.load_breast_cancer()                  # Loading the Breast Cancer dataset\n",
    "\n",
    "\n",
    "# Splitting the dataset into features (X) and target variable (y)\n",
    "X, y = data.data, data.target                        # Assigning features (X) and target variable (y) from the dataset\n",
    "\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1234\n",
    ")  \n",
    "# Splitting X and y into train and test subsets with 80% for training and 20% for testing\n",
    "\n",
    "\n",
    "# Creating an instance of the DecisionTree class with a maximum depth of 10\n",
    "clf = DecisionTree(max_depth=10)                   # Creating an instance of the DecisionTree class with max_depth=10\n",
    "\n",
    "\n",
    "# Training the Decision Tree classifier on the training data\n",
    "clf.fit(X_train, y_train)                         # Fitting the classifier on the training data\n",
    "\n",
    "\n",
    "# Making predictions on the test data\n",
    "predictions = clf.predict(X_test)                 # Predicting the labels for the test data using the trained classifier\n",
    "\n",
    "\n",
    "# Defining a function to calculate accuracy\n",
    "def accuracy(y_test, y_pred):\n",
    "    # Calculating the accuracy as the proportion of correct predictions\n",
    "    return np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "\n",
    "# Calculating the accuracy of the model on the test data\n",
    "acc = accuracy(y_test, predictions)             # Calculating the accuracy of the model\n",
    "print(acc)                                      # Printing the accuracy of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35504875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
